# -*- coding: utf-8 -*-
"""d-tree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xz5KuVlwkcjKSXOl_bk0i1Qj7J5RKKOJ
"""

import pandas as pd
import numpy as np
from sklearn import tree
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt

# criterion_type can be gini or entropy
def decision_tree(input_address, criterion_type='gini'):

    # read data from excel and convert to numpy array.
    df = pd.read_excel(input_address).values
    # get precent of each columns  that is missed.
    number_of_zeros = np.sum((df[:, :-1] == 0), axis=0) / len(df)
    print('precent of each column that is missed')
    for _ in range(df.shape[1]-1):
        print('column', _+1, number_of_zeros[_])

    # fill missing value by 1 or -1 if number of occurance of 1 is 
    # bigger than -1 we fill with 1 otherwise fill with -1
    # separate data from labels.
    corrected_values = df[:, :-1].tolist()

    # fill missed values.
    for _ in range(df.shape[1]-1):
        number_of_one = np.sum((df[:, _] == 1))
        number_of_mines_one = np.sum((df[:, _] == -1))
        if number_of_one > number_of_mines_one:
            fill_value = 1
        else:
            fill_value = -1

        for r in range(df.shape[0]):
            if corrected_values[r][_] == 0:
                corrected_values[r][_] = fill_value

    x_train, x_test , y_train, y_test = train_test_split(
        corrected_values,
        df[:, -1],
        test_size=.2
    )
    clf = tree.DecisionTreeClassifier(criterion=criterion_type, random_state=0)
    clf.fit(x_train, y_train)
    print('####### Accuray with ',criterion_type, 'is ', clf.score(x_test, y_test))
    return clf

clf_gini = decision_tree('./Train.xlsx')
print("-----------------------------------------")
print('################## Tree for gini')
plt.figure(figsize=(40,40))
tree.plot_tree(clf_gini)

clf_entropy = decision_tree('./Train.xlsx', 'entropy')
print('tree for entropy')
plt.figure(1)
plt.figure(figsize=(40,40))
tree.plot_tree(clf_entropy)
plt.show()